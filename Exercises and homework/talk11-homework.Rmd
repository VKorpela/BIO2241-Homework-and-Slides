---
title: "talk11 练习与作业"
documentclass: ctexart
output:
  rticles::ctex:
    fig_caption: yes
    number_sections: yes
    toc: true
    toc_depth: 2
  word_document: default
  html_document:
    df_print: paged
knit: (
  function(inputFile, encoding) { 

    pSubTitle <- 'talk11-homework'

    rmarkdown::render( 
      input       = inputFile, 
      encoding    = encoding, 
      params      = list(sub_title = pSubTitle),      
      output_file = pSubTitle) })
---

## 练习和作业说明

将相关代码填写入以 \`\`\`{r} \`\`\` 标志的代码框中，运行并看到正确的结果；

完成后，用工具栏里的"Knit"按键生成PDF文档；

**将PDF文档**改为：**`姓名-学号-talk11作业.pdf`**，并提交到老师指定的平台/钉群。

## talk11 内容回顾

待写 .. 

## 练习与作业：用户验证

请运行以下命令，验证你的用户名。

**如你当前用户名不能体现你的真实姓名，请改为拼音后再运行本作业！**

```{r}
Sys.info()[["user"]]
Sys.getenv("HOME")
```


## 练习与作业1：linear regression

------------------------------------------------------------------------

### **一元回归分析**

用 `readr` 包的函数将 `Excercises and homework/data/talk11/` 目录下的 `income.data_.zip` 文件装入到 `income.dat` 变量中，进行以下分析：

1.  用线性回归分析 `income` 与 `happiness` 的关系；
2.  用点线图画出`income` 与 `happiness` 的关系，将推导出来的公式写在图上；
3.  用得到的线性模型，以`income`为输入，预测`happiness`的值；
4.  用点线图画出预测值与真实`happiness`的关系，并在图上写出 R2 值。

```{r}
## 代码写这里，并运行；
library(tidyverse)
library(caret)
income.dat <- read_csv("./data/talk11/income.data_.zip")

# 1. 用线性回归分析关系
income.lm <- lm(income.dat$happiness ~ income.dat$income)

# 2. 绘制income和happiness关系
eq_text <- substitute(
  italic(y) == a %.% italic(x) + b,
  list(
    a = format(coef(income.lm)[[2]], digits = 2),
    b = format(coef(income.lm)[[1]], digits = 2)
  )
) %>%
  as.expression() %>%
  as.character()

ggplot(income.dat, aes(x = income, y = happiness)) +
  geom_point() +
  geom_smooth(method = "lm") +
  geom_label(
    data = NULL,
    aes(x = 2.5, y = 6, label = eq_text),
    parse = TRUE,
    inherit.aes = FALSE
  )

# 3. 预测happiness值
happiness.pred <- predict(income.lm, income.dat['income'])

# 4. 绘制预测值与真实值的关系
happiness.cmp <- data.frame(
  ref = income.dat$happiness,
  pred = happiness.pred
)

happiness.r2 <- with(happiness.cmp, R2(pred, ref))
r2_text <- substitute(
  italic(r)^2 == r2,
  list(r2 = format(happiness.r2, digits = 4))
) %>%
  as.expression() %>%
  as.character()

ggplot(happiness.cmp, aes(x = ref, y = pred)) +
  geom_point() +
  geom_smooth(method = "lm") +
  xlab("Reference") +
  ylab("Prediction") +
  geom_label(
    data = NULL,
    aes(x = 1, y = 5, label = r2_text),
    parse = TRUE,
    inherit.aes = FALSE
  )
```

------------------------------------------------------------------------

### **多元回归分析**

用 `readr` 包的函数将 `Excercises and homework/data/talk11/` 目录下的 `heart.data_.zip` 文件装入到 `heart.dat` 变量中，进行以下分析：

1.  用线性回归分析 `heart.disease` 与 `biking` 和 `smoking` 的关系；
2.  写出三者间关系的线性公式；
3.  解释`biking` 和 `smoking`的影响（方向和程度）；
4.  `biking` 和 `smoking`能解释多少`heart.disease`的variance？这个值从哪里获得？
5.  用 `relaimpo`包的函数计算`biking` 和 `smoking`对`heart.disease`的重要性。哪个更重要？
6.  用得到的线性模型预测`heart.disease`，用点线图画出预测值与真实值的关系，并在图上写出 R2 值。
7.  在建模时考虑 `biking` 和 `smoking`的互作关系，会提高模型的 R2 值吗？如果是，意味着什么？如果不是，又意味着什么？

```{r}
## 代码写这里，并运行；
library(relaimpo)
heart.dat <- read_csv("./data/talk11/heart.data_.zip")

# 1. 线性回归分析
heart.lm <- lm(
  heart.disease ~ biking + smoking,
  data = heart.dat
)

# 2. 线性回归公式
heart.coef <- coef(heart.lm)

paste0(
  "heart.disease = ",
  format(heart.coef["biking"], digits = 3),
  " * biking + ",
  format(heart.coef["smoking"], digits = 3),
  " * smoking + ",
  format(heart.coef[1], digits = 3)
)

# 3. biking和smoking的影响
# 由系数可见，biking与heart.disease负相关
# 而smoking与heart.disease正相关
# 二者的影响程度接近，相对而言biking更大
heart.coef

# 4. 两个系数对变化的解释
# 根据R^2的值，可以得到结论：
# biking能解释87.5%的variance
# smoking能解释9.6%的variance
with(
  heart.dat,
  data.frame(
    biking = R2(biking, heart.disease),
    smoking = R2(smoking, heart.disease)
  )
)

# 5. 计算重要性
# 结论：biking比smoking更重要
calc.relimp(heart.disease ~ biking + smoking, data = heart.dat)

# 6. 预测
heart.pred <- predict(heart.lm, heart.dat)

heart.comp <- data.frame(
  ref = heart.dat$heart.disease,
  pred = heart.pred
)

heart.pred.r2 <- with(heart.comp, R2(ref, pred))
r2_text <- substitute(
  italic(r)^2 == r2,
  list(r2 = format(heart.pred.r2, digits = 4))
) %>%
  as.expression() %>%
  as.character()

ggplot(heart.comp, aes(x = ref, y = pred)) +
  geom_point() +
  geom_smooth(method = "lm") +
  xlab("Reference") +
  ylab("Prediction") +
  geom_label(
    data = NULL,
    aes(x = 3, y = 17, label = r2_text),
    parse = TRUE,
    inherit.aes = FALSE
  )

# 7. 考虑互作关系
# 结论：在建模时考虑互作关系，不会明显提高模型的R2，
# 这说明收集到的数据中biking与smoking并不存在强关联。
heart.lm2 <- lm(
  heart.disease ~ biking * smoking,
  data = heart.dat
)

data.frame(
  no_interaction = summary(heart.lm)$r.squared,
  with_interaction = summary(heart.lm2)$r.squared
)
```

------------------------------------------------------------------------

### **`glm` 相关问题**

用 `glm` 建模时使用`family=binomial`；在预测时， `type=`参数可取值 `link`（默认）和 `response`。请问，两者的区别是什么？请**写代码**举例说明。

```{r}
## 代码写这里，并运行；
iris.dat <- iris %>% filter(Species %in% c("setosa", "virginica"))
iris.binom <- glm(Species ~ Sepal.Length + Sepal.Width + 
                  Petal.Length + Petal.Width,
                  data = iris.dat, family = binomial)

# 当type取值为link时，给出的分类是log-odds
# （即通过logit函数表示的属于某一类的概率）
# 当type取值为response时，给出的则是直接的概率值
data.frame(
  ref = iris.dat$Species,
  pred.link = predict(iris.binom, iris.dat, type = "link"),
  pred.response = predict(iris.binom, iris.dat, type = "response")
) %>%
  sample_n(6) %>%
  arrange(ref)
```

## 练习与作业2：non-linear regression

------------------------------------------------------------------------

### **分析 `swiss` ，用其它列的数据预测`Fertility`**

1.  使用`earth`包建模，并做 10 times 10-fold cross validation;
2.  使用`lm`方法建模，同样做 10 times 10-fold cross validation;
3.  用 `RMSE` 和 `R2` 两个指标比较两种方法，挑选出较好一个；
4.  用 `vip` 包的函数查看两种方法中 feature 的重要性，并画图（如下图所示）：

![](images/talk11-feature-importance.png)

```{r}
## 代码写这里，并运行；
library(earth)
library(vip)
set.seed(1129)

# 1. 使用earth包建模
cv.mars <- train(
  Fertility ~ .,
  data = swiss,
  method = "earth",
  trControl = trainControl(method = "cv", number = 10)
)
cv.mars

# 2. 使用lm建模
cv.lm <- train(
  Fertility ~ .,
  data = swiss,
  method = "lm",
  trControl = trainControl(method = "cv", number = 10)
)
cv.lm

# 3. 比较RMSE和R^2的指标
# 可以得到线性模型（lm）更优的结论
data.frame(
  name = c("MARS", "LM"),
  r.squared = c(
    cv.mars[["results"]][["Rsquared"]][2],
    cv.lm[["results"]][["Rsquared"]]
  ),
  RMSE = c(
    cv.mars[["results"]][["RMSE"]][2],
    cv.lm[["results"]][["RMSE"]]
  )
)

# 4. 查看feature重要性
plot.mars <- vip(cv.mars, num_features = 5, geom = "point", value = "gcv") +
  ggtitle("MARS: GCV")
plot.lm <- vip(cv.lm, num_features = 5, geom = "point", value = "gcv") +
  ggtitle("LM: GCV")

gridExtra::grid.arrange(plot.mars, plot.lm, ncol = 2)
```